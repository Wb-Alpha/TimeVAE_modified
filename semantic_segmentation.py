import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy.signal import find_peaks
from scipy.stats import linregress
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³matplotlibä¸­æ–‡ä¹±ç é—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def analyze_and_extract_similar_cycles(dat_file, output_dir, num_cycles=5):
    """
    åˆ†æè®¾å¤‡æ•°æ®ï¼Œæå–ç›¸ä¼¼çš„å·¥ä½œå‘¨æœŸ
    """
    print(f"\nğŸ” Analyzing: {dat_file}")

    # è¯»å–æ•°æ®
    timestamps, powers = read_power_data(dat_file)

    if not powers:
        print(f"  âš ï¸  File is empty: {dat_file}")
        return None, None

    print(f"  Data points: {len(powers)}")
    print(f"  Power range: {min(powers)} - {max(powers)} W")

    # è¯†åˆ«å®Œæ•´å‘¨æœŸ
    cycles = identify_complete_cycles(powers, timestamps)

    if not cycles:
        print(f"  âš ï¸  No complete cycles found")
        return None, None

    print(f"  Found {len(cycles)} complete cycles")

    # è¯„ä¼°å‘¨æœŸè´¨é‡å¹¶é€‰æ‹©æœ€ä½³å‘¨æœŸ
    best_cycles = select_best_cycles(cycles, num_cycles)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    device_name = Path(dat_file).stem
    device_output_dir = Path(output_dir) / device_name
    device_output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é€‰å®šçš„å‘¨æœŸ
    save_cycles_as_npz(best_cycles, device_output_dir, device_name)

    return best_cycles, device_name


def read_power_data(dat_file):
    """è¯»å–åŠŸç‡æ•°æ®"""
    timestamps = []
    powers = []

    try:
        with open(dat_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    timestamps.append(int(parts[0]))
                    powers.append(int(parts[1]))
    except UnicodeDecodeError:
        # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
        with open(dat_file, 'r', encoding='latin-1') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    timestamps.append(int(parts[0]))
                    powers.append(int(parts[1]))

    return timestamps, powers


def extract_pattern_features(power_window):
    """æå–åŠŸç‡æ¨¡å¼ç‰¹å¾"""
    if len(power_window) < 2:
        return None

    # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
    mean_power = np.mean(power_window)
    std_power = np.std(power_window)

    # è¶‹åŠ¿ç‰¹å¾
    x = np.arange(len(power_window))
    try:
        slope, _, _, _, _ = linregress(x, power_window)
    except:
        slope = 0

    # å³°å€¼ç‰¹å¾
    try:
        peaks, _ = find_peaks(power_window, height=max(mean_power * 0.5, 5), distance=5)
        peak_count = len(peaks)
    except:
        peak_count = 0

    # é›¶åŠŸç‡ç‰¹å¾
    zero_ratio = np.sum(np.array(power_window) < 5) / len(power_window)

    # åŠŸç‡æ°´å¹³ç‰¹å¾
    power_levels = identify_power_levels(power_window)

    return {
        'mean': mean_power,
        'std': std_power,
        'slope': slope,
        'peak_count': peak_count,
        'zero_ratio': zero_ratio,
        'power_levels': power_levels
    }


def identify_power_levels(power_window, threshold=10):
    """è¯†åˆ«ä¸»è¦çš„åŠŸç‡æ°´å¹³"""
    # è¿‡æ»¤æ‰å¾®å°æ³¢åŠ¨
    significant_powers = [p for p in power_window if p >= threshold]
    if not significant_powers:
        return []

    # ä½¿ç”¨K-meansè¯†åˆ«åŠŸç‡æ°´å¹³
    if len(significant_powers) >= 3:
        power_array = np.array(significant_powers).reshape(-1, 1)
        n_clusters = min(3, len(set(significant_powers)))
        if n_clusters > 1:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            kmeans.fit(power_array)
            return sorted([int(center[0]) for center in kmeans.cluster_centers_])

    return [int(np.mean(significant_powers))]


def identify_working_modes(power_series, min_pattern_length=30):
    """è¯†åˆ«å·¥ä½œæ¨¡å¼"""
    patterns = []
    window_size = min_pattern_length
    step_size = max(window_size // 3, 1)  # é‡å çª—å£ï¼Œç¡®ä¿è‡³å°‘ä¸º1

    for i in range(0, len(power_series) - window_size + 1, step_size):
        window = power_series[i:i + window_size]

        # åªå¤„ç†åŒ…å«æ˜¾è‘—åŠŸç‡çš„çª—å£
        if np.max(window) < 10:  # å¿½ç•¥ä½åŠŸç‡çª—å£
            continue

        features = extract_pattern_features(window)
        if features:
            patterns.append({
                'start_idx': i,
                'end_idx': i + window_size,
                'features': features,
                'data': window
            })

    return patterns


def cluster_similar_patterns(patterns, n_clusters=5):
    """èšç±»ç›¸ä¼¼æ¨¡å¼"""
    if len(patterns) < n_clusters:
        n_clusters = max(1, len(patterns))

    feature_vectors = []
    for pattern in patterns:
        features = pattern['features']
        vector = [
            features['mean'],
            features['std'],
            features['slope'],
            features['peak_count'],
            features['zero_ratio']
        ]
        # æ·»åŠ åŠŸç‡æ°´å¹³ç‰¹å¾
        if features['power_levels']:
            # ç¡®ä¿å‘é‡é•¿åº¦ä¸€è‡´
            for level in features['power_levels'][:3]:  # æœ€å¤šå–3ä¸ªåŠŸç‡æ°´å¹³
                vector.append(level)
            # å¦‚æœåŠŸç‡æ°´å¹³ä¸è¶³3ä¸ªï¼Œå¡«å……0
            while len(vector) < 8:  # 5ä¸ªåŸºç¡€ç‰¹å¾ + æœ€å¤š3ä¸ªåŠŸç‡æ°´å¹³
                vector.append(0)
        else:
            vector.extend([0, 0, 0])  # å¡«å……3ä¸ª0

    feature_vectors.append(vector)

    # ç¡®ä¿æ‰€æœ‰å‘é‡é•¿åº¦ä¸€è‡´
    max_len = max(len(v) for v in feature_vectors)
    for i, v in enumerate(feature_vectors):
        if len(v) < max_len:
            feature_vectors[i] = v + [0] * (max_len - len(v))

    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(feature_vectors)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(scaled_features)

    for i, pattern in enumerate(patterns):
        pattern['cluster'] = labels[i]

    return patterns, kmeans


def merge_patterns_to_cycle(patterns, timestamps):
    """å°†è¿ç»­ç›¸ä¼¼æ¨¡å¼åˆå¹¶ä¸ºå®Œæ•´å‘¨æœŸ"""
    start_idx = patterns[0]['start_idx']
    end_idx = patterns[-1]['end_idx']

    # å»é™¤é‡å¤éƒ¨åˆ†ï¼ˆç”±äºé‡å çª—å£ï¼‰
    unique_powers = []
    current_idx = start_idx
    for pattern in patterns:
        pattern_start = pattern['start_idx']
        pattern_end = pattern['end_idx']

        if pattern_start >= current_idx:
            # æ·»åŠ éé‡å éƒ¨åˆ†
            if pattern_start > current_idx:
                # æ·»åŠ é—´éš™æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                pass
            unique_powers.extend(pattern['data'])
            current_idx = pattern_end

    # å¦‚æœåˆå¹¶åæ•°æ®å¤ªé•¿ï¼Œé€‚å½“æˆªæ–­
    if len(unique_powers) > 1000:
        unique_powers = unique_powers[:1000]
        end_idx = start_idx + 1000

    return {
        'start_idx': start_idx,
        'end_idx': end_idx,
        'start_time': timestamps[start_idx],
        'end_time': timestamps[min(end_idx, len(timestamps) - 1)],
        'duration': timestamps[min(end_idx, len(timestamps) - 1)] - timestamps[start_idx],
        'timesteps': len(unique_powers),
        'power_values': unique_powers,
        'patterns': patterns,
        'cluster': patterns[0]['cluster'],
        'pattern_count': len(patterns)
    }


def identify_complete_cycles(power_data, timestamps):
    """è¯†åˆ«å®Œæ•´å·¥ä½œå‘¨æœŸ"""
    patterns = identify_working_modes(power_data)

    if len(patterns) < 3:  # è‡³å°‘éœ€è¦3ä¸ªæ¨¡å¼æ‰èƒ½æœ‰æ•ˆè¯†åˆ«å‘¨æœŸ
        return []

    patterns, kmeans = cluster_similar_patterns(patterns)

    cycles = []
    current_cycle = []
    current_cluster = None

    for pattern in patterns:
        if current_cluster is None:
            current_cluster = pattern['cluster']
            current_cycle.append(pattern)
        elif pattern['cluster'] == current_cluster:
            # æ£€æŸ¥è¿ç»­æ€§ï¼ˆæ¨¡å¼ä¹‹é—´ä¸èƒ½æœ‰å¤ªå¤§é—´éš”ï¼‰
            if pattern['start_idx'] - current_cycle[-1]['end_idx'] <= 100:  # æœ€å¤§é—´éš”
                current_cycle.append(pattern)
            else:
                # é—´éš”å¤ªå¤§ï¼Œç»“æŸå½“å‰å‘¨æœŸ
                if len(current_cycle) >= 2:
                    cycle = merge_patterns_to_cycle(current_cycle, timestamps)
                    if cycle['timesteps'] >= 50:  # ç¡®ä¿å‘¨æœŸè¶³å¤Ÿé•¿
                        cycles.append(cycle)
                current_cluster = pattern['cluster']
                current_cycle = [pattern]
        else:
            # ç°‡å˜åŒ–ï¼Œç»“æŸå½“å‰å‘¨æœŸ
            if len(current_cycle) >= 2:
                cycle = merge_patterns_to_cycle(current_cycle, timestamps)
                if cycle['timesteps'] >= 50:  # ç¡®ä¿å‘¨æœŸè¶³å¤Ÿé•¿
                    cycles.append(cycle)
            current_cluster = pattern['cluster']
            current_cycle = [pattern]

    if len(current_cycle) >= 2:
        cycle = merge_patterns_to_cycle(current_cycle, timestamps)
        if cycle['timesteps'] >= 50:  # ç¡®ä¿å‘¨æœŸè¶³å¤Ÿé•¿
            cycles.append(cycle)

    return cycles


def calculate_pattern_similarity(patterns):
    """è®¡ç®—æ¨¡å¼ç›¸ä¼¼åº¦"""
    if len(patterns) < 2:
        return 0

    similarities = []
    for i in range(len(patterns) - 1):
        feat1 = patterns[i]['features']
        feat2 = patterns[i + 1]['features']

        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        mean_diff = abs(feat1['mean'] - feat2['mean'])
        mean_sim = 1 - mean_diff / max(feat1['mean'], feat2['mean'], 1)

        std_diff = abs(feat1['std'] - feat2['std'])
        std_sim = 1 - std_diff / max(feat1['std'], feat2['std'], 1)

        similarities.append((mean_sim + std_sim) / 2)

    return np.mean(similarities)


def calculate_regularity(cycle):
    """è®¡ç®—å‘¨æœŸè§„å¾‹æ€§"""
    patterns = cycle['patterns']
    if len(patterns) < 3:
        return 0

    # è®¡ç®—æ¨¡å¼æŒç»­æ—¶é—´çš„è§„å¾‹æ€§
    durations = [p['end_idx'] - p['start_idx'] for p in patterns]
    duration_std = np.std(durations)
    max_duration = max(durations)
    regularity = 1 - (duration_std / max_duration) if max_duration > 0 else 0

    return regularity


def select_best_cycles(cycles, num_cycles):
    """é€‰æ‹©æœ€ä½³å‘¨æœŸ"""
    scored_cycles = []

    for cycle in cycles:
        pattern_similarity = calculate_pattern_similarity(cycle['patterns'])
        regularity = calculate_regularity(cycle)

        # å‘¨æœŸé•¿åº¦ä¹Ÿæ˜¯ä¸€ä¸ªè€ƒé‡å› ç´ 
        length_score = min(1.0, cycle['timesteps'] / 200)  # åå¥½è¾ƒé•¿å‘¨æœŸ

        quality_score = (pattern_similarity * 0.4 +
                         regularity * 0.3 +
                         length_score * 0.3)

        cycle['quality_score'] = quality_score
        cycle['pattern_similarity'] = pattern_similarity
        cycle['regularity'] = regularity

        scored_cycles.append(cycle)

    # æŒ‰è´¨é‡æ’åºå¹¶é€‰æ‹©æœ€ä½³å‘¨æœŸ
    best_cycles = sorted(scored_cycles, key=lambda x: x['quality_score'], reverse=True)[:num_cycles]

    print(f"  Selected {len(best_cycles)} best cycles:")
    for i, cycle in enumerate(best_cycles):
        print(f"    Cycle {i + 1}: {cycle['timesteps']} steps, "
              f"{cycle['pattern_count']} patterns, "
              f"quality: {cycle['quality_score']:.3f}")

    return best_cycles


def save_cycles_as_npz(cycles, output_dir, device_name):
    """ä¿å­˜å‘¨æœŸæ•°æ®"""
    if not cycles:
        return None

    # å°†æ‰€æœ‰å‘¨æœŸæ•°æ®ç»„åˆæˆä¸€ä¸ªnumpyæ•°ç»„ (n_cycles, timesteps, 1)
    all_cycles_data = []
    max_timesteps = max(cycle['timesteps'] for cycle in cycles)

    for i, cycle in enumerate(cycles):
        power_data = np.array(cycle['power_values'])

        if len(power_data) < max_timesteps:
            padded_data = np.pad(power_data, (0, max_timesteps - len(power_data)),
                                 mode='constant', constant_values=0)
            all_cycles_data.append(padded_data)
        else:
            all_cycles_data.append(power_data)

    data_array = np.array(all_cycles_data).reshape(len(cycles), max_timesteps, 1)

    # ä¿å­˜æ–‡ä»¶
    filename = f"{device_name}_similar_cycles.npz"
    filepath = output_dir / filename
    np.savez(filepath, data=data_array)

    print(f"  ğŸ’¾ Saved: {filename} (shape: {data_array.shape})")

    # ä¿å­˜å‘¨æœŸä¿¡æ¯
    cycle_info = {
        'device': device_name,
        'total_cycles': len(cycles),
        'timesteps_per_cycle': max_timesteps,
        'cycle_timesteps': [cycle['timesteps'] for cycle in cycles],
        'pattern_counts': [cycle['pattern_count'] for cycle in cycles],
        'quality_scores': [cycle['quality_score'] for cycle in cycles],
        'pattern_similarities': [cycle['pattern_similarity'] for cycle in cycles]
    }

    info_file = output_dir / "cycle_info.txt"
    with open(info_file, 'w', encoding='utf-8') as f:
        f.write(f"Device: {cycle_info['device']}\n")
        f.write(f"Total cycles: {cycle_info['total_cycles']}\n")
        f.write(f"Timesteps per cycle: {cycle_info['timesteps_per_cycle']}\n")
        f.write(f"Original cycle timesteps: {cycle_info['cycle_timesteps']}\n")
        f.write(f"Pattern counts: {cycle_info['pattern_counts']}\n")
        f.write(f"Quality scores: {cycle_info['quality_scores']}\n")
        f.write(f"Pattern similarities: {cycle_info['pattern_similarities']}\n")
        f.write(f"\nNPZ file shape: {data_array.shape}\n")

    return cycle_info


def generate_summary_report(all_cycles_info, output_dir):
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    """
    print(f"\n{'=' * 60}")
    print("ğŸ“Š Processing Complete - Summary Report")
    print(f"{'=' * 60}")

    summary_data = []

    for device_name, cycles_info in all_cycles_info.items():
        if cycles_info:
            summary_data.append({
                'Device': device_name,
                'Cycles': cycles_info['total_cycles'],
                'Timesteps': cycles_info['timesteps_per_cycle'],
                'Original Timesteps': cycles_info['cycle_timesteps'],
                'Pattern Counts': cycles_info['pattern_counts'],
                'Quality Scores': [f"{score:.3f}" for score in cycles_info['quality_scores']],
                'Pattern Similarities': [f"{sim:.3f}" for sim in cycles_info['pattern_similarities']]
            })

            print(f"\n{device_name}:")
            print(f"  Extracted cycles: {cycles_info['total_cycles']}")
            print(f"  Final timesteps: {cycles_info['timesteps_per_cycle']}")
            print(f"  Original timesteps: {cycles_info['cycle_timesteps']}")
            print(f"  Pattern counts: {cycles_info['pattern_counts']}")
            print(f"  Quality scores: {cycles_info['quality_scores']}")
            print(f"  Pattern similarities: {cycles_info['pattern_similarities']}")

    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š - ä½¿ç”¨UTF-8ç¼–ç 
    summary_file = Path(output_dir) / "processing_summary.csv"
    df = pd.DataFrame(summary_data)
    df.to_csv(summary_file, index=False, encoding='utf-8-sig')  # ä½¿ç”¨UTF-8å¸¦BOMç¼–ç 
    print(f"\nğŸ’¾ Summary report saved: {summary_file}")


def plot_sample_segments(all_cycles_info, output_dir):
    """
    ç»˜åˆ¶æ ·æœ¬ç‰‡æ®µçš„åŠŸç‡æ›²çº¿ - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…å­—ä½“é—®é¢˜
    """
    # ä¸ºæ¯ä¸ªè®¾å¤‡åˆ›å»ºä¸€ä¸ªå›¾è¡¨
    for device_name, cycles_info in all_cycles_info.items():
        if not cycles_info:
            continue

        # åŠ è½½æ•°æ®
        data_file = Path(output_dir) / device_name / f"{device_name}_similar_cycles.npz"
        if data_file.exists():
            data = np.load(data_file)
            all_cycles = data['data']

            # åˆ›å»ºå›¾è¡¨
            n_cycles = min(5, len(all_cycles))
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            axes = axes.flatten()

            # ç»˜åˆ¶å‰å‡ ä¸ªå‘¨æœŸ
            for i in range(n_cycles):
                if i < len(axes):
                    power_series = all_cycles[i, :, 0]
                    time_axis = np.arange(len(power_series))

                    # åªç»˜åˆ¶éé›¶éƒ¨åˆ†
                    nonzero_indices = np.where(power_series > 0)[0]
                    if len(nonzero_indices) > 0:
                        start_idx = max(0, nonzero_indices[0] - 5)
                        end_idx = min(len(power_series), nonzero_indices[-1] + 5)
                        plotted_power = power_series[start_idx:end_idx]
                        plotted_time = time_axis[start_idx:end_idx]
                    else:
                        plotted_power = power_series
                        plotted_time = time_axis

                    axes[i].plot(plotted_time, plotted_power, 'b-', linewidth=1.5)
                    axes[i].set_title(f'{device_name} - Cycle {i + 1}\n{len(power_series)} steps')
                    axes[i].set_xlabel('Time Steps')
                    axes[i].set_ylabel('Power (W)')
                    axes[i].grid(True, alpha=0.3)

            # éšè—å¤šä½™çš„å­å›¾
            for i in range(n_cycles, len(axes)):
                axes[i].set_visible(False)

            plt.tight_layout()
            plt.savefig(Path(output_dir) / device_name / f"{device_name}_cycles_plot.png",
                        dpi=300, bbox_inches='tight')
            plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…æ˜¾ç¤º

    # åˆ›å»ºä¸€ä¸ªæ±‡æ€»å›¾è¡¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªè®¾å¤‡çš„ç¬¬ä¸€ä¸ªå‘¨æœŸ
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    device_count = 0
    for device_name, cycles_info in all_cycles_info.items():
        if device_count >= len(axes) or not cycles_info:
            continue

        # åŠ è½½æ•°æ®
        data_file = Path(output_dir) / device_name / f"{device_name}_similar_cycles.npz"
        if data_file.exists():
            data = np.load(data_file)
            first_cycle = data['data'][0, :, 0]

            # åªç»˜åˆ¶éé›¶éƒ¨åˆ†
            nonzero_indices = np.where(first_cycle > 0)[0]
            if len(nonzero_indices) > 0:
                start_idx = max(0, nonzero_indices[0] - 5)
                end_idx = min(len(first_cycle), nonzero_indices[-1] + 5)
                plotted_power = first_cycle[start_idx:end_idx]
                plotted_time = np.arange(len(plotted_power))
            else:
                plotted_power = first_cycle
                plotted_time = np.arange(len(first_cycle))

            ax = axes[device_count]
            ax.plot(plotted_time, plotted_power, 'b-', linewidth=1.5)
            ax.set_title(f'{device_name}\n{len(first_cycle)} steps')
            ax.set_xlabel('Time Steps')
            ax.set_ylabel('Power (W)')
            ax.grid(True, alpha=0.3)

            device_count += 1

    # éšè—å¤šä½™çš„å­å›¾
    for i in range(device_count, len(axes)):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "all_devices_first_cycle.png", dpi=300, bbox_inches='tight')
    plt.show()


def fix_existing_encoding_issues():
    """
    ä¿®å¤å·²å­˜åœ¨çš„ç¼–ç é—®é¢˜æ–‡ä»¶
    """
    processed_dir = Path("./processed_data")

    if not processed_dir.exists():
        return

    # æŸ¥æ‰¾æ‰€æœ‰cycle_info.txtæ–‡ä»¶å¹¶é‡æ–°ä¿å­˜ä¸ºUTF-8
    for info_file in processed_dir.glob("**/cycle_info.txt"):
        try:
            # å°è¯•ç”¨ä¸åŒç¼–ç è¯»å–
            with open(info_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç”¨UTF-8é‡æ–°å†™å…¥
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"âœ… Fixed encoding: {info_file}")

        except UnicodeDecodeError:
            # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
            for encoding in ['gbk', 'latin-1', 'cp1252']:
                try:
                    with open(info_file, 'r', encoding=encoding) as f:
                        content = f.read()

                    with open(info_file, 'w', encoding='utf-8') as f:
                        f.write(content)

                    print(f"âœ… Fixed encoding ({encoding} -> UTF-8): {info_file}")
                    break
                except:
                    continue


def main():
    """
    ä¸»å‡½æ•°ï¼šå¤„ç†æ‰€æœ‰è®¾å¤‡æ•°æ®
    """
    # é…ç½®è·¯å¾„
    data_dir = "./pre_data"  # åŸå§‹æ•°æ®ç›®å½•
    output_dir = "./data"  # å¤„ç†åçš„æ•°æ®ç›®å½•

    # è®¾å¤‡æ–‡ä»¶åˆ—è¡¨
    devices = [
        "fridge.dat",
        "washing_machine.dat",
    ]

    print("ğŸš€ Starting to process all device data...")
    print(f"Input directory: {data_dir}")
    print(f"Output directory: {output_dir}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # è®¾å¤‡ç‰¹å®šçš„å‚æ•°ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    device_params = {
        "laptop": {"min_pattern_length": 20, "num_cycles": 5},
        "kettle": {"min_pattern_length": 10, "num_cycles": 5},
        "microwave": {"min_pattern_length": 15, "num_cycles": 5},
        "washing_machine": {"min_pattern_length": 30, "num_cycles": 5},
        "dishwasher": {"min_pattern_length": 25, "num_cycles": 5}
    }

    all_cycles_info = {}

    # å¤„ç†æ¯ä¸ªè®¾å¤‡
    for device_file in devices:
        device_path = Path(data_dir) / device_file

        if not device_path.exists():
            print(f"âš ï¸  Skipping: {device_path} does not exist")
            continue

        device_name = device_path.stem
        params = device_params.get(device_name, {"min_pattern_length": 25, "num_cycles": 5})

        # è¿™é‡Œç®€åŒ–è°ƒç”¨ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ ¹æ®paramsè°ƒæ•´ç®—æ³•å‚æ•°
        cycles, device_name = analyze_and_extract_similar_cycles(
            device_path,
            output_dir,
            num_cycles=params["num_cycles"]
        )

        if cycles:
            # ä¿å­˜å‘¨æœŸä¿¡æ¯
            cycle_info = {
                'total_cycles': len(cycles),
                'timesteps_per_cycle': max(cycle['timesteps'] for cycle in cycles),
                'cycle_timesteps': [cycle['timesteps'] for cycle in cycles],
                'pattern_counts': [cycle['pattern_count'] for cycle in cycles],
                'quality_scores': [cycle['quality_score'] for cycle in cycles],
                'pattern_similarities': [cycle['pattern_similarity'] for cycle in cycles]
            }
            all_cycles_info[device_name] = cycle_info

    # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
    generate_summary_report(all_cycles_info, output_dir)
    plot_sample_segments(all_cycles_info, output_dir)

    # ä¿®å¤å¯èƒ½å­˜åœ¨çš„ç¼–ç é—®é¢˜
    fix_existing_encoding_issues()

    print(f"\nğŸ‰ All processing completed!")
    print(f"ğŸ“ Data saved in: {output_dir}")
    print(f"ğŸ“Š Each device has 5 similar cycles in a single NPZ file")
    print(f"ğŸ“‹ Summary report: {output_dir}/processing_summary.csv")
    print(f"ğŸ–¼ï¸  Sample charts: {output_dir}/[device_name]/[device_name]_cycles_plot.png")


if __name__ == "__main__":
    main()